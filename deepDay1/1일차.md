### Stacking Linear Functions
- Linear Function은 Layer를 깊게 쌓아도 수식을 풀어쓰면 1차로 표현 가능하기 때문에 깊게 쌓는게 의미가 없다.

### Adding Non-Linearity
- Linear layer의 아웃풋에 activation function을 조합함으로써 어려운 문제를 표현할 수 있을것 이다. 라는 가정
- 다루는 함수는 인풋 아웃풋은 어떻게 돌아갈지 모르지만 결정된거를 줘라
- Linear가 Optimal이면 Linear로 학습해
- 자유도 있는 학습

### Learning Deep Neural Network
#### Loss function
- 얼마나 네트워크가 잘 동작하고 있나
- Loss를 무엇무엇하게 할 것 (minimize, maximize, sometimes)
- Loss가 합리적으로, 미분가능하게 잘 디자인이 되어야 한다.
- Multiclass SVM Loss for classification : http://ishuca.tistory.com/378


